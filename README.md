# Home_sales

Final homework assignment

In this challenge we were to create a dataframe from the home_sales_revised data and make a temporary table from the df. Then we needed perform 4 queries building upon the previous until the fourth query is to show a runtime. Next we create a cached version and validated. This is rerun with another runtime being taken. We then partition the dataset by the date_built and the formatted parquet data is read. Another temporary table is created, this time from the parquet data. Lastly we create another runtime and compare from the previous. This is wrapped up by uncaching the temporary table and verifying.

This challenge showcased the utilization of SparkSQL for accessing, querying, caching, and analyzing datasets related to home sales. By leveraging SparkSQL capabilities, the project offered valuable insights into average prices across different criteria, thereby enhancing the understanding of the housing market dynamics.
